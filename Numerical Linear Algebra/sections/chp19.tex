\chapter{Stability of Least Squares Algorithms} 
Least squares problems can be solved by various methods, as described in Chapter 11, including the normal equations, Householder triangularization, GramSchmidt orthogonalization, and the SVD. Here we compare these methods and show that the use of the normal equations is in general unstable.

\section{Example} 
We consider the following example. We will fit $\exp(\sin (4\tau))$ on the interval $[0,1]$ by a polynomial of degree $14$.  

\begin{lstlisting}[language=Matlab]
 m=100; n=15;
 t= (0:m-1)'/(m-1); &\Comment{ Set $t$ to a discretization of $[0,1]$. } &
 A = []; for i=1:n; &\Comment{ Construct Vandermonde matrix. } &
    A = [A t.^(i-1)]; end 
 b = exp(sin(4*t));  &\Comment{  Right-hand side  } &
 b = b/2006.787453080206;  &\Comment{ Make the coefficient $c_{15}=1$ } &
\end{lstlisting}
Now we check the quantities: 
\begin{lstlisting}[language=Matlab]
 x = A\b; y = A*x; &\Comment{ Solve LS problem. } &
 kappa = cond(A) 
    kappa = kappa = 2.2718e+10 &\Comment{ $\kappa(A)$ } &   
 theta = asin(norm(b-y)/norm(b)) 
    theta = 3.7461e-06 &\Comment{ $\theta$ } &
 eta = norm(A)*norm(x)/norm(y)
    eta = 2.1036e+05 &\Comment{ $\eta$ } &
\end{lstlisting}

The result $\kappa(A) \approx 10^{10}$ indicates that the monomials $1, t, \ldots, t^{14}$ form a highly ill-conditioned basis. The result $\theta \approx 10^{-6}$ indicates that $\exp (\sin (4 t))$ can be fitted very closely by a polynomial of degree 14 . (The fit is so close that we computed $\theta$ with the formula $\theta=\sin ^{-1}(\|b-y\| /\|b\|)$, to avoid cancellation error.) As for $\eta$, its value of about $10^5$ is about midway between the extremes 1 and $\kappa(A)$. 

We can get the following table: 
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline 
    & $y$ &$x$ \\ 
    \hline 
    $b$ & $ 1.0 $ & $1.1\times 10^5$ \\ 
    \hline 
    $A$ & $2.3\times 10^{10}$ & $3.2\times 10^{10}$ \\ 
    \hline
    \end{tabular}
\end{table}

\newpage
\section{Householder Triangularization} 
The code is 
\begin{lstlisting}[language=Matlab ]
 [Q,R] = qr(A,0); &\Comment{ Householder triang. of $A$. } &
 x = R\(Q'*b); &\Comment{ Solve for $x$. } &
 x(15)
    ans = 1.00000031528723
\end{lstlisting}
The relative error is $3\times 10^{-7}$ and $\mep = 10^{-16}$ and the rounding errors have been amplified by a factor of order $10^9$. It can be entirely explained by ill-conditioning. Hence, Algo~\ref{Algo 11.2} appears to be backward stable.  

Note that we don't really have to store $\hat Q$ but the vectors $v_k$. Then, we can compute $\hat Q^*b$ by Algo~\ref{Algo 10.2}. In MATLAB, we can achieve this effect by computing a QR factorization not just of $A$ but of the $m \times(n+1)$ "augmented" matrix $\left[\begin{array}{ll}A & b\end{array}\right]$. In the course of this factorization, the $n$ Householder reflectors that make $A$ upper-triangular are applied to $b$ also, leaving the vector $\hat{Q}^* b$ in the first $n$ positions of column $n+1$. An additional $(n+1)$ st reflector is then applied to make entries $n+2, \ldots, m$ of column $n+1$ zero, but this does not change the first $n$ entries of that column, which are the ones we care about.

\begin{lstlisting}[language=Matlab]
 [Q2, R2] = qr([A b], 0); &\Comment{ Householder triang. of $[A b]$ } &
 R2 = R2(1:n, 1:n);
 Qb = R2(1:n, n+1);
 x = R2\Qb;
 x(15)
    ans = 1.00000031529465
\end{lstlisting}
The answer is the same.  

A third way is to use the built-in operator $ \setminus$ via Householder triangularization.

\begin{lstlisting}[language=Matlab ]
 x = A\b;
 x(15)
    ans = 0.99999994311087
\end{lstlisting}
The result is better due to column pivoting! 

We have the following theorem: 

%────────────────────────────────────────
\begin{theorem}
[Backstap of Householder for LS]
\label{thm: Backstap of Householder for LS}
Let the full-rank least squares problem \eqref{eq: least sq} be solved by Householder triangularization (Algorithm~\ref{Algo 11.2}) on a computer satisfying \eqref{eq: fler} and \eqref{eq:flop}. This algorithm is backward stable in the sense that the computed solution $\tilde{x}$ has the property
\begin{align}
    \label{eq: backstap of LS}
\|(A+\delta A) \tilde{x}-b\|=\min , \quad \frac{\|\delta A\|}{\|A\|}=O\left(\epsilon_{\text {machine}}\right)
\end{align}
for some $\delta A \in \mathbb{C}^{m \times n}$. This is true whether $\hat{Q}^* b$ is computed via explicit formation of $\hat{Q}$ or implicitly by Algorithm~\ref{Algo 10.2}. It also holds for Householder triangularization with arbitrary column pivoting.
\end{theorem}
%────────────────────────────────────────

\section{GS Orthogonalization}
Another way to solve a least squares problem is by modified Gram-Schmidt orthogonalization (Algorithm~\ref{Algo 8.1}). For $m \approx n$, this takes somewhat more operations than the Householder approach, but for $m \gg n$, the flop counts for both algorithms are asymptotic to $2 m n^2$. We have the following code: 

\begin{lstlisting}[language=Matlab]
 [Q, R] = mgs(A);
 x = R \ (Q'*b);
 x(15)
    ans = 1.02926594532672
\end{lstlisting}
This result is very poor. Rounding errors have been amplified by a factor on the order of $10^{14}$, far greater than the condition number of the problem. In fact, this algorithm is unstable, and the reason is easily identified. As mentioned at the end of Lecture 9, Gram-Schmidt orthogonalization produces matrices $\hat{Q}$, in general, whose columns are not accurately orthonormal. Since the algorithm above depends on that orthonormality, it suffers accordingly.

A better method of stabilizing the Gram-Schmidt method is to make use of an augmented system of equations, just as in the second of our two Householder experiments above:
\begin{lstlisting}[language=Matlab]
 [Q2,R2] = mgs([A b]); 
 R2 = R2(1:n,1:n);
 Qb = R2(1:n,n+1);
 x = R2\Qb;
 x(15)
    ans = 1.00000005653399
\end{lstlisting}
Now the result looks as good as with Householder triangularization. It can be proved that this is always the case.


%────────────────────────────────────────
\begin{theorem}
[Backstap of GS for LS]
\label{thm: Backstap of GS for LS}
The solution of the full-rank least squares problem \eqref{eq: least sq} by Gram-Schmidt orthogonalization is also backward stable, satisfying \eqref{eq: backstap of LS}, provided that $\hat{Q}^* b$ is formed implicitly as indicated in the code segment above.
\end{theorem}
%────────────────────────────────────────

\section{Normal Equations} 
A fundamentally different approach to least squares problems is the solution of the normal equations (Algorithm~\ref{Algo 11.1}), typically by Cholesky factorization (Chapter 23). For $m \gg n$, this method is twice as fast as methods depending on explicit orthogonalization, requiring asymptotically only $m n^2$ flops. In the following experiment, the problem is solved in a single line of MATLAB by the $\setminus$ operator:
\begin{lstlisting}[language=Matlab]
 x = (A'*A)\(A'*b);&\Comment{ Form and solve normal equations. } &
 x(15)
    ans = 0.39339069870283
\end{lstlisting}
This result is terrible! It is the worst we have obtained, with not even a single digit of accuracy. The use of the normal equations is clearly an unstable method for solving least squares problems. 

Suppose we have a backward stable algorithm for the full-rank LS problem that delivers a solution $\tilde{x}$ satisfying $\|(A+\delta A) \tilde{x}-b\|=\min$ for some $\delta A$ with $\|\delta A\| /\|A\|=O\left(\epsilon_{\text {machine }}\right)$. (Allowing perturbations in $b$ as well as $A$, or considering stability instead of backward stability, does not change our main points.) By Thm~\ref{thm: Accuracy of a back stap algo} of and Thm~\ref{thm: Conditioning of LS} we have
\[
    \frac{\|\tilde{x}-x\|}{\|x\|}=O\left(\left(\kappa+\frac{\kappa^2 \tan \theta}{\eta}\right) \epsilon_{\text {machine }}\right),
\]
where $\kappa = \kappa(A)$.  Now suppose $A$ is ill-conditioned, i.e., $\kappa \gg 1$, and $\theta$ is bounded away from $\pi / 2$. Depending on the values of the various parameters, two very different situations may arise. If $\tan \theta$ is of order 1 (that is, the least squares fit is not especially close) and $\eta \ll \kappa$, the right-hand side is $O\left(\kappa^2 \epsilon_{\text {machine }}\right)$. On the other hand, if $\tan \theta$ is close to zero (a very close fit) or $\eta$ is close to $\kappa$, the bound is $O\left(\kappa \epsilon_{\text {machine }}\right)$. The condition number of the least squares problem may lie anywhere in the range $\kappa$ to $\kappa^2$.

Now consider what happens when we solve LS by the normal equations, $\left(A^* A\right) x=A^* b$. Cholesky factorization is a stable algorithm for this system of equations in the sense that it produces a solution $\tilde{x}$ satisfying $\left(A^* A+\delta H\right) \tilde{x}=$ $A^* b$ for some $\delta H$ with $\|\delta H\| /\left\|A^* A\right\|=O\left(\epsilon_{\text {machine }}\right)$ (Theorem 23.3). However, the matrix $A^* A$ has condition number $\kappa^2$, not $\kappa$. Thus the best we can expect from the normal equations is
\begin{align*}
\frac{\|\tilde{x}-x\|}{\|x\|}=O\left(\kappa^2 \epsilon_{\text {machine }}\right) .
\end{align*}
The behavior of the normal equations is governed by $\kappa^2$, not $\kappa$.

The conclusion is now clear. If $\tan \theta$ is of order 1 and $\eta \ll \kappa$, or if $\kappa$ is of order 1, then two bounds are of the same order and the normal equations are stable. If $\kappa$ is large and either $\tan \theta$ is close to zero or $\eta$ is close to $\kappa$, however, then the previous one is much bigger than the good bound and the normal equations are unstable. The normal equations are typically unstable for ill-conditioned problems involving close fits. In our example problem, with $\kappa^2 \approx 10^{20}$, it is hardly surprising that Cholesky factorization yielded no correct digits.

According to our definitions, an algorithm is stable only if it has satisfactory behavior uniformly across all the problems under consideration. The following result is thus a natural formalization of the observations just made.


%────────────────────────────────────────
\begin{theorem}
[Stability of normal equations]
\label{thm: Stability of normal equations}
The solution of the full-rank least squares problem \eqref{eq: least sq} via the normal equations (Algorithm~\ref{Algo 11.1}) is unstable. Stability can be achieved, however, by restriction to a class of problems in which $\kappa(\dot{A})$ is uniformly bounded above or $(\tan \theta) / \eta$ is uniformly bounded below.
\end{theorem}
%────────────────────────────────────────

\section{SVD}
SVD is stable. 
\begin{lstlisting}[language=Matlab]
 [U,S,V] = svd(A,0); 
 x = V*(S\(U'*b));
 x(15)
    ans = 0.99999998230471
\end{lstlisting}

In fact, this is the most accurate of all the results obtained in our experiments, beating Householder triangularization with column pivoting (MATLAB's $\backslash$ ) by a factor of about 3. A theorem in the usual form can be proved.


%────────────────────────────────────────
\begin{theorem}
[Backstap of SVD]
\label{thm: Backstap of SVD}
The solution of the full-rank least squares problem \eqref{eq: least sq} by the SVD (Algorithm~\ref{Algo 11.3}) is backward stable, satisfying the estimate~\eqref{eq: backstap of LS}.
\end{theorem}
%────────────────────────────────────────

\section{Rank-Deficient LS Problems} 
In this chapter we have identified four backward stable algorithms for linear least squares problems: Householder triangularization, Householder triangularization with column pivoting, modified Gram-Schmidt with implicit calculation of $\hat{Q}^* b$, and the SVD. From the point of view of classical normwise stability analysis of the full-rank problem~\ref{eq: least sq} the differences among these algorithms are minor, so one might as well make use of the simplest and cheapest, Householder triangularization without pivoting.

However, there are other kinds of least squares problems where column pivoting and the SVD take on a special importance. These are problems where $A$ has rank $<n$, possibly with $m<n$, so that the system of equations is underdetermined. Such problems do not have a unique solution unless one adds an additional condition, typically that $x$ itself should have as small a norm as possible. A further complication is that the correct solution depends on the rank of $A$, and determining ranks numerically in the presence of rounding errors is never a trivial matter.

Thus rank-deficient least squares problems are not a challenging subclass of least squares problems, but fundamentally different. Since the definition of a solution is new, there is no reason that an algorithm that is stable for full-rank problems must be stable also in the rank-deficient case. In fact, the only fully stable algorithms for rank-deficient problems are those based on the SVD. An alternative is Householder triangularization with column pivoting, which is stable for almost all problems. We shall not give details.