\chapter{The Arnoldi Iteration}
Despite the many names and acronyms that have proliferated in the field of Krylov subspace matrix iterations, these algorithms are built upon a common foundation of a few fundamental ideas. One can take various approaches to describing this foundation. Ours will be to consider the Arnoldi process, a Gram—Schmidt-style iteration for transforming a matrix to Hessenberg form.

\section{The Arhnoli/Gram-Shmidt Analogy} 
The goal of Arnoldi iteration is to reduce a nonhermitian matrix to Hessenberg form by orthogonal transformations, proceeding column by column from a prescribed first column $ q_1 $.  

We have discussed two methods for QR factorization: 
\begin{itemize}
    \item Householder reflections, triangularize $ A $ by a succession of orthogonal operations;
    \item Gram-Schmidt orthogonalization, orthogonalize $A$ by a succession of triangular operations.
\end{itemize}
Note that the GS process has the advantage that it can be stopped aprt-way, leaving one with a reduced QR factorization of the first $ n $ columns of $ A $. The problem of computing a Hessenberg reduction $ A= QHQ^*  $ of a matrix $ A $ is exactly analogous. There are two standard methods: House holder reflections and the Arnoldi iteration.  

\section{Mechanics of the Arnoldi Iteration} 
Give n $ A \in \RR^{m\times m} $, an Hessenberg form is $ A= QHQ^*  $.  However, in dealing with iterative methods we take the view that $ m $ is huge or infinite, so that computing the full reduction os out of question.  Let $ Q^{n}\in \RR^{m\times n}$ be the first $ n $ columns of $ Q $ and $ Q = (q_1,q_2,\ldots ,q_n) $.    Let $ \tilde H_n\in \RR^{(n+1)\times n} $ be the upper-left section of $ H $, which is also Hessenberg: 
\begin{equation}
\label{eq: tilde H}
    H = \begin{bmatrix}[] 
        h_{11} &  & \cdots &    h_{1n} \\
        h_{21} & h_{ 22 }  &    &\vdots    \\
         & \ddots & \ddots &    \vdots \\
         &  & h_{n,n-1} &h_{nn}     \\
         &  &  & h_{n+1, n}    \\
    \end{bmatrix} . 
\end{equation}
 Then we have 
 \[
    AQ_n = Q_{ n+1}  \tilde H_n.  
 \]
 The $ n $th column of this equation ca nbe written as follows: 
 \begin{equation}
 \label{eq: Hessenberg form}
    Aq_n = h_{1n}q_1 +\cdots + h_{nn}q_n + h_{n+1,n}q_{n+1}. 
 \end{equation}

 Then, we have the Arnoldi iteration:

 \begin{algorithm}[H]
     \caption{Arnoldi Iteration}
     \label{Algo 33.1}
     random $ b $, $ q_1 = \frac{b}{\|b\|} $\; 
     \For{$ n=1,2,3,\ldots  $}{
        $ v= A q_{n} $\; 
        \For{$ j=1 $ \KwTo $ n $}{
            $ h_{jn} = q_j^*  v $\; 
            $ v = v - h_{jn}q_j $\; 
        }
        $ h_{n+1,n} = \|v\| $\; 
        $ q_{n+1} = v / h_{n+1,n} $
     }
 \end{algorithm}
 
 Note that the matrix $ A $ only appears in the product $ A q_n $, which can be computed by a black box procedure. 
 
 \section{QR Factorization of a Krylov Matrix} 
  
 It is evident from this formula that the vectors $\{q_j\}  $ form bases of the successive Krylov subspaces generated by $ A $ and $ b $, defined as follows: 
\[
    \cK_n = \langle b, Ab, \ldots ,A^{n-1}b \rangle  = \langle q_1,q_2,\ldots ,q_n \rangle \subseteq \CC^{m}. 
\]
It's obvious that $\{q_j\} _{j=1}^n$ is an orthonormal basis for $ \cK_n $.  We define $ K_n \in \RR^{m \times n} $ as $ K_n = (b, Ab, \ldots ,A^{n-1}b) $. Then, $ K_n $ must a reduced QR factorization, 
\begin{equation}
\label{eq: QR of K_n}
    K_n = Q_n R_n,
\end{equation}
where $ Q_n $ is the same matrix as above.  This give an intuition explanation of why the Arnoldi process leads to effective methods for determining certain eigenvalues. Clearly $K_n$ might be expected to contain good information about the eigenvalues of $ A $ with largest modules, and the QR factorization might be expected to reveal this information by peeling off one approximate eigenvector after another.  

\section{Projection onto Krylov Subspaces}

 Another way to view the Arnoldi process is as a computation of projections onto successive Krylov subspaces. Note that $ Q_n^*Q_{n+1} $ is the $ n\times (n+1)$ identity, and 
 \[
    H_n = Q_n^* Q_{n+1}\tilde H_n = \begin{bmatrix}[] 
        h_{11} & \cdots & \cdots &  h_{1n} \\
        h_{21} & h_{22}  &  &\vdots   \\
         &\ddots  &\ddots  &\vdots   \\
         &  & h_{n,n-1}  & h_{nn}  \\
    \end{bmatrix} \in \RR^{n\times n}.
 \]
 Hence, 
 \[
    H_n = Q_n^* Q_{n+1}\tilde H_n = Q_n^* A Q_n. 
 \]
 This matrix can be interpreted as the representation in the basis $\left\{q_1, \ldots, q_n\right\}$ of the orthogonal projection of $A$ onto $\mathcal{K}_n$. Is it clear what this interpretation means? Here is a precise statement. Consider the linear operator $\mathcal{K}_n \rightarrow \mathcal{K}_n$ defined as follows: given $v \in \mathcal{K}_n$, apply $A$ to it, then orthogonally project $A v$ back into the space $\mathcal{K}_n$. Since the orthogonal projector of $\mathbb{C}^m$ onto $\mathcal{K}_n$ is $Q_n Q_n^*$, this operator can be written $Q_n Q_n^* A$ with respect to the standard basis of $\mathbb{C}^m$. With respect to the basis of columns of $Q_n$, it can therefore be written $Q_n^* A Q_n$.

 Since $ H_n $ is a projection of $ A $, one might imagine that its eigenvalues would be related to those of $ A $ in a useful fashion. These $ n $ numbers,
 \begin{equation}
 \label{eq: eigenvalues of H_n}
    \{\theta _j\}  = \{\text{ eigenvalues of  }H_n\} , 
 \end{equation}
 are called the \textbf{Arnoldi eigenvalue estimates} or \textbf{Ritz values} of $ A $.  

 
 %────────────────────────────────────────
 \begin{theorem}
 [Arnoldi iteration]
 \label{thm: Arnoldi iteration}
 The matrices $ Q_n $ generated by the Arnoldi iteration are reduced QR factors of the Krylov matrix: 
 \begin{equation}
 \label{eq: QR Arnoldi}
    K_n =Q_n R_n.
 \end{equation}
 The Hessenberg matrices $ H_n $ are the corresponding projections, 
 \begin{equation}
 \label{eq: Arnoldi projection}
    H_n = Q_n^* A Q_n,
 \end{equation}
 and the successive iterates are related by the formula 
 \begin{equation}
 \label{eq: Arnoldi iterates}
    AQ_n =Q_{n+1}\tilde H_n. 
 \end{equation}
 
 
 \end{theorem}
 %────────────────────────────────────────
 