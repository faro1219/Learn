\chapter{Solving implicit methods} 
For LMM of the form \eqref{eq: LMM}, which we reproduce here: 
\[
    \sum_{j=0}^{r} \alpha _j x_{n+j} = h \sum_{j=0}^{r} \beta _j f_{n+j}.   
\]
Here we assume $ \alpha _r = 1 $ and $ \beta _r \neq 0 $. Then 
\[
    x_{n+r} = \sum_{j=0}^{r-1} (-\alpha _j) x_{n+j} + h \beta _r f(x_{n+r}, t_{n+r}) + \sum_{j=0}^{r-1} h\beta _j f_{n+j}.   
\]

For fixed $ x_n,\ldots ,x_{n+r-1}$, define the function $ \Phi: \RR^{d}\to \RR^{d} $ by 
\[
    \Phi(u) \coloneqq \sum_{j=0}^{r-1} (-\alpha _j) x_{n+j} + h \beta _r f(u, t_{n+r})  + \sum_{j=0}^{r-1} h\beta _j f_{n+j}.   
\]
Then to solve the implicit scheme for $ x_{n+r} $, given all preceding values $ x_n,\ldots ,x_{n+r-1}$, we must solve the (generally nonlinear) system of equations
\begin{equation}
\label{eq: solve implicit method}
    u = \Phi(u). 
\end{equation}
Or, defining $ F(u)\coloneqq  u - \Phi(u) $, we may write even more abstractly 
\begin{equation}
\label{eq: solve implicit method root}
    F(u) = 0.
\end{equation}
General approaches to solving such systems of equations are \textit{iterative}, in that sense that a solution $ u^\star $ is furnished as a limit
\[
    u^\star = \lim_{k\to \infty}u^{(k)}, 
\]
where each successive iterate $ u^{(k+1)} $ can be feasibly computed from the last iterate $ u^{(k)} $. 


%────────────────────────────────────────
\begin{definition}
[Linear convergence]
\label{def: Linear convergence}
We say that an iterative converges linearly with rate $ \alpha \in (0,1) $ if 
\[
    |u^{(k)} - u ^*| = O(\alpha ^{(k)}). 
\]
\end{definition}
%────────────────────────────────────────
You might think that we should say that such a method converges 'exponentially,' but in fact it is much less classy to do so. It is the logarithm of the error that converges linearly, and it is important to stay humble, especially since, as we shall see it is always possible in principle to do much better (though perhaps at great cost). Indeed, if a method converges linearly, this means precisely that each successive digit of accuracy is just as costly to produce as the last digit. If a method converges superlinearly, then each successive digit comes more easily, and once we are close to a solution, we tend to get to machine precision in a hurry.

\section{Picard iteration}
Solving \eqref{eq: solve implicit method root} is equivalent to looking for a fixed point $ \Phi $. And we can use the Picard iteration. 
\[
    u^{(k+1)} = \Phi (u^{(k)}), 
\] 
for arbitrary initialization $ u^{(0)} $. Now if $ f $ is $ L $-Lipschitz, then we have 
\[
    \begin{aligned}
        |\Phi(u)-\Phi(v)| & =h\left|\beta_r\right|\left|f\left(u, t_{n+r}\right)-f\left(v, t_{n+r}\right)\right| \\
        & \leq L\left|\beta_r\right| h|u-v|,
        \end{aligned}
\]
 so if $ h<\frac{1}{(L|\beta _r|)} $, then indeed $ \Phi $ is a contraction mapping with a unique fixed point. 

 