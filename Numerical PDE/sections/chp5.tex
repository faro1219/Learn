\part{Linear Multistep Methods}
\chapter{Linear Multistep Methods}
 

Given the solution $x(t)$ at time $t$ of \eqref{eq: IVP} we already encountered two different ways of approximately advancing the solution by one increment $h$ of time. The first was
$$
x(t+h) \approx x(t)+h f(x(t), t)
$$
which led to the explicit Euler method, and the second was
$$
x(t+h) \approx x(t)+h f(x(t+h), t+h)
$$
which led to the implicit Euler method.
One can readily imagine a mixture of the two, approaches:
$$
x(t+h) \approx x(t)+\frac{1}{2}[f(x(t), t)+f(x(t+h), t+h)],
$$
which in fact has one higher order of accuracy, as we shall see.
The resulting numerical schemes were written compactly as
$$
\begin{array}{lr}
x_{n+1}-x_n=h f_n & \text { (Explicit Euler) } \\
x_{n+1}-x_n=h f_{n+1} & \text { (Implicit Euler) } \\
x_{n+1}-x_n=h\left(f_n+f_{n+1}\right) & \text { (Trapezoidal rule) }
\end{array}
$$
where as always $f_n=f\left(x_n, t_n\right)$ implicitly depends on $x_n$. Linear multistep methods (LMMs) generalize these schemes considerably. 
%────────────────────────────────────────
\begin{definition}
[Linear multistep methods]
\label{def: Linear multistep methods}
The general form of an r-step LMM is
\begin{equation}
\label{eq: LMM}
    \sum_{j=0}^r \alpha_j x_{n+j}=h \sum_{j=0}^r \beta_j f_{n+j} .
\end{equation}
Assuming that $x_m$ is known for $m=0, \ldots, n+r-1$, this equation can in principle be solved for the next value $x_{n+r}$.
\end{definition}
%────────────────────────────────────────
Note that explicit Euler, implicit Euler and trapezoidal rule are special cases of LMM. For convenience, we always assume $ \alpha _r =1$. If $ \beta_r=0 $, this method will be explicit. Otherwise, it will be implicit.  

\section{Local truncation error} 
In terms of \autoref{eg: LTE of Explicit Euler }, the LTE for LMM is: 
\[
    \tau^n \coloneqq \frac{1}{h} \sum_{j=0}^{r}  \alpha _j x_{n+j} - \sum_{j=0}^{r} \beta _j f_{n+j}.   
\] 
Besides, the one-step error is: 
\[
    \cL^{n} =\sum_{j=0}^{r}  \alpha _j x_{n+j} - h\sum_{j=0}^{r} \beta _j f_{n+j}. 
\]
Note that we can also define a linear operator for a differential function $ z $, 
\begin{equation}
\label{eq: linear operator as one-step error }
        \mathscr{L}_h z(t) \coloneqq  \sum_{j=0}^r \alpha_j z(t+j h)-h \sum_{j=0}^r \beta_j z^{\prime}(t+j h) .
\end{equation}


%────────────────────────────────────────
\begin{proposition}
[LTE for 3 methods]
\label{prop: LTE for 3 methods}
\begin{itemize}
    \item []
    \item For Explicit Euler, $ \mathscr{L}_h z(t) = \frac{h^{2} }{2}z^{\prime \prime }(t) +O(h^3) $; 
    \item For Implicit Euler, $ \mathscr{L}_h z(t) = -\frac{h^{2} }{2}z^{\prime \prime }(t) +O(h^3) $; 
    \item For trapezoidal rule, $ \mathscr{L}_h z(t) = \frac{h^{3} }{12}z^{\prime \prime  \prime}(t) +O(h^4) $.
\end{itemize}
\end{proposition}
%────────────────────────────────────────

Hence, the trapezodial rule is second order accurate.  


\section{Consistency} 


%────────────────────────────────────────
\begin{definition}
[Consistency]
\label{def: Consistency}
We say that an LMM is consistent of order $p$ or order- $p$ consistent if $\mathscr{L}_h z(t)=O\left(h^{p+1}\right)$ for every smooth $z$. In particular, we say that it is consistent if it is order- $p$ consistent for some $p \geq 1$. Moreover, if 
\[
    \mathscr{L}_h z(t)=C_{p+1} h^{p+1} z^{(p+1)}(t)+O\left(h^{p+2}\right),
\]
where $C_{p+1} \neq 0$. This nonzero coefficient $C_{p+1}$ is called the error constant of the LMM. 
\end{definition}
%────────────────────────────────────────
In general, the error constant can in principle be computed analytically, and it may be useful to do so! Indeed, Milne's device (to be discussed later) uses error constants to estimate the error of an LMM and possibly improve it.

 It is not hard to derive necessary and sufficient conditions for the consistency of \eqref{eq: LMM}.
 
 
 %────────────────────────────────────────
 \begin{theorem}
 [Consistency of LMM]
 \label{thm: Consistency of LMM}
An LMM \eqref{eq: LMM} is consistent if and only if
$$
\sum_{j=0}^r \alpha_j=\sum_{j=0}^r\left(j \alpha_j-\beta_j\right)=0 .
$$
 \end{theorem}
 %────────────────────────────────────────
 %────────────────────────────────────────
 \begin{proof}
 Note that 
 \[
    \begin{aligned}
        \mathscr{L}_h z(t) & =\sum_{j=0}^r \alpha_j z(t+j h)-h \sum_{j=0}^r \beta_j z^{\prime}(t+j h) \\
        & =\sum_{j=0}^r \alpha_j\left[z(t)+j h z^{\prime}(t)\right]-h \sum_{j=0}^r \beta_j\left[z^{\prime}(t)\right]+O\left(h^2\right) \\
        & =\left(\sum_{j=0}^r \alpha_j\right) z(t)+h\left(\sum_{j=0}^r j \alpha_j-\beta_j\right) z^{\prime}(t)+O\left(h^2\right).
        \end{aligned}
 \]
 In fact, a more general expansion is
 \[
    \mathscr{L}_h z(t) =\left(\sum_{j=0}^r \alpha_j\right) z(t)+ \sum_{q=1}^{p-1} h^{q} \left( \sum_{j=0}^r \left( \frac{1}{q!}j^{q}\alpha _j - \frac{1}{(q-1)!}j^{q-1}\beta _{j} \right) \right) z^{(q)}(t) + O(h^{p}).
 \]
 Hence, we can get the sufficient and necessary condition for any order $ p $. 
 \end{proof}
 %────────────────────────────────────────

 
 %────────────────────────────────────────
 \begin{definition}
 [first and second characteristic polynomials of LMM]
 \label{def: first and second characteristic polynomials of LMM}
 The first and the second characteristic polynomials are: 
 \[
    \rho(w)=\sum_{j=0}^r \alpha_j w^j, \quad \sigma(w)=\sum_{j=0}^r \beta_j w^j .
 \]

 \end{definition}
 %────────────────────────────────────────
 
 
 %────────────────────────────────────────
 \begin{corollary}
 \label{cor: characteristic poly LMM consistency}
 An LMM is consistent if and only if 
 \[
    \rho (1) = 0, \rho ^\prime (1) = \sigma (1). 
 \]
 \end{corollary}
 %────────────────────────────────────────
 As we shall later see, consistency does not imply convergence! There is an additional stability requirement that we shall treat later.
 
 \section{Starting values} 
 Note that to solve an $r$-step LMM, we need $r$ starting values $x_0, \ldots, x_{r-1}$ in order to initialize the scheme. This is annoying! We are only given $x_0=x(0)$ from the initial condition of \eqref{eq: IVP}.

 The most naive option is to simply take $x_j=x_0$ for $j=1, \ldots, r-1$. However, this might sacrifice the order of accuracy of our scheme. Let us try to get some heuristic understanding of what we require.


 In general, assume we have $x\left(t_j\right)=x_0+\Theta(h)$ for $j=0, \ldots, r-1$, since $x(t)$ is differentiable at $t=0$. Therefore under the naive approach, our error $e_j:=x_j-x\left(t_j\right)$ is already $\Theta(h)$ by time step $j=r-1$. If our scheme is order- $p$ consistent in the sense that the one-step error is $O\left(h^{p+1}\right)$, we will accumulate an additional error that is $O\left(h^{p+1}\right)$ at each of the remaining $O\left(h^{-1}\right)$ time steps (cf. the proof of Theorem 11, conceptually), i.e., we will accumulate additional error of $O\left(h^p\right)$. However, this additional error accumulation is dominated by our $\Theta(h)$ initialization error, and the entire method is really only first-order consistent. If $p=1$, this is no worse than expected, and we can adopt the naive approach without too much regret. 

 More generally, the preceding argument suggests that we need to initialize $x_j=x\left(t_j\right)+$ $O\left(h^p\right)$ in order to fulfill the dream of order- $p$ accuracy for an LMM with $O\left(h^{p+1}\right)$ one-step error. We could determine these intitial values, for example, by running $r-1$ steps of the order- $(p-1)$ accurate Taylor series method TS $(p-1)$. Since the number of steps needed for initialization is independent of the step size $h$, despite the disadvantages of Taylor series methods this may be a reasonable practical approach. In order to solve the ODE up to time $T$, we will need to run $\sim h^{-1}$ steps of the LMM, so as $h \rightarrow 0$, the initialization cost should be negligible.

 Still, it may be more practically efficient to use Runge-Kutta methods, which do not require starting values, for initialization. (Runge-Kutta methods will be the subject of the next chapter of the note.)
 