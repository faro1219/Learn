\chapter{QR Algorithm with Shifts} 

What makes the QR iteration fly is the introduction of shifts $A\to A-\mu I$ at each step. Here we explain how this idea leads to cubic convergence, thanks to an implicit connection with Rayleigh quotient iteration.  

\section{Connection with Inverse Iteration} 
As we have seen, the ``pure'' algorithm (Algo ~\ref{Algo 28.1}) is equivalent to simultaneous iteration applied to the identity matrix, and in particular the fist column of the result evolves according to the power iteration applied to $ e_1 $. There is a dual to this observation. Algo~\ref{Algo 28.1} is also equivalent to \textbf{simultaneous inverse iteration} applied to a ``flipped'' identity matrix $P$, and in particular, the $ m $th column of the result evolves according to inverse iteration applied to $ e_m $.  

Recall that we have $Q^{(k)}$ and 
\[
    \underline Q^{(k)} = \prod_{j=1}^{k} Q^{(j)} = \left[ \begin{array}{c|c|c|c}
        & & & \\ 
        q_1^{(k)} & q_2 ^{(k)} & \cdots & q_m^{(k)}\\ 
        & & & 
    \end{array} \right]. 
\]
Besides, we have $\underline Q^{(k)}$ is the orthogonal factor in a QR factorization, 
\[
    A^{k} = \underline Q^{(k)} \underline R^{(k)}. 
\]
Now we can invert this formula. We have 
\[
    A^{-k} = \left( \underline{R}^{(k)} \right) ^{-1} \underline{Q}^{(k)\top} = \underline{Q}^{(k)}\left( \underline{R}^{(k)} \right) ^{-\top}. 
\]
Let 
\[
    P = \begin{bmatrix}[] 
         &  &  &  1 \\
         &  & 1 &   \\
         & \cdots &  &   \\
        1 &  &  &   \\
    \end{bmatrix}.  
\]
Since $P^{2} =I$, we have 
\[
    A^{-k}P = [ \underline{Q}^{(k)}P] [P( \underline{R}^{(k)})^{-\top}P]. 
\]
Note that $\underline{Q}^{(k)}P$ is orthogonal and $ P(\underline{R}^{(k)}) ^{-\top}P $ is upper-triangular.  Hence, we get a QR factorization of $ A^{-k}P $.  In other words, we are effectively carrying out simultaneous iteration on $A^{-1} $ applied to the initial matrix $ P $, which is to say, simultaneous inverse iteration on $ A $. In particular--the last column of $ \underline{Q}^{(k)} $--is the result of applying $ k $ steps of inverse iteration to the vector $ e_m $. 

\section{Connection with Shifted Inverse Iteration} 
Thus the QR algorithm is both simultaneous iteration and simultaneous inverse iteration.  Then, we can add shifts to accelerate the QR algorithm.  Algo~\ref{Algo 28.2} showed how shifts are introduced into a step of the QR algorithm. Doing this corresponds exactly to shifts in the corresponding simultaneous iteration and inverse iteration processes, and their beneficial effect is therefore exactly the same. 

Let $\mu^{(k)}$ denote the eigenvalue estimate chosen at the $k$th step of the QR algorithm. From Algo~\ref{Algo 28.2}, we have 
\begin{align*}
    A^{(k-1)} - \mu^{(k)}I &= Q^{(k)}R^{(k)}, \\ 
    A^{(k)} &= R^{(k)} Q^{(k)} + \mu^{(k)} I . 
\end{align*}
This implies, 
\[
    A^{(k)} = (Q^{(k)}) ^\top  A^{(k-1)}Q^{(k)}, 
\]
and by induction, 
\[
    A^{(k)} = (\underline{Q}^{(k)})^\top  A \underline{Q}^{(k)},
\]
which is the same to \eqref{eq: A bk projection QR}. However \eqref{eq: Qr sim iter k} no longer holds. Instead, we have 
\[
    (A-\mu^{(k)}I ) (A-\mu ^{(k-1)}I) \cdots (A-\mu^{(1)}I) = \underline{Q}^{(k)}\underline{R}^{(k)}.  
\]

\section{Connection with Rayleigh Quotient Iteration} 
We have discovered a powerful tool hidden in the shifted QR algorithm: shifted inverse iteration. To complete the idea, we now need a way of choosing shifts to achieve fast convergence in the last column of $ \underline{Q}^{(k)} $.  

We can use the Rayleigh quotient. To estimate the eigenvalue corresponding to the eigenvector approximated by the last column of $\underline{Q}^{k}$, it's natural to apply the Rayleigh quotient. This gives us 
\[
    \mu ^{(k)} = \frac{(q_m^{(k)})^\top A q_m^{(k)}}{(q_m^{(k)})^\top q_m^{(k)}} = (q_m^{(k)})^\top  A q_m^{(k)}. 
\]
If this number is chosen as the shift at every step, the eigenvalue and eigenvector estimates $ \mu^{(k)} $ and $ q_m^{(k)} $ are identical to those that are computed by the Rayleigh quotient iteration starting with $ e_m $.  Therefore, the QR algorithm has cubic convergence in the sense that $ q_m^{(k)} $ converges cubically to an eigenvector.  

Notice that, in the QR algorithm, the Raleigh quotient appears as the $ (m,m) $ entry of $A^{(k)}$. 
\[
    A^{(k)}_{mm} = e_m^\top  A^{(k)}e_m = e_m^\top  \underline{Q^{(k)\top}} A \underline{Q}^{(k)} e_m = q_m^{(k)\top} A q_m^{(k)}. 
\]
Therefore, we can simply set $ \mu^{(k)}= A_{mm} ^{(k)} $. This is known as the \textbf{Rayleigh quotient shift}. 

\section{Wilkinson Shift}
Although the Rayleigh quotient shift gives cubic convergence in the generic case, convergence is not guaranteed for all initial conditions. Consider 
\[
    A = \begin{bmatrix}[] 
        0 &  1 \\
        1 &  0 \\
    \end{bmatrix}.  
\] 
The unshifted QR algorithm does not converge at all: 
\begin{align*}
    A &= Q^{(1)}R^{(1)} = \begin{bmatrix}[] 
        0 & 1 \\
        1 &  0 \\
    \end{bmatrix} \begin{bmatrix}[] 
        1 &  0 \\
        0 &  1 \\
    \end{bmatrix}, \\ 
    A^{(1)} &= R^{(1)}Q^{(1)} = \begin{bmatrix}[] 
        1 &  0 \\
        0 &  1 \\
    \end{bmatrix}\begin{bmatrix}[] 
        0 &  1 \\
        1 &  0 \\
    \end{bmatrix} = A.   
\end{align*}

The Rayleigh quotient shift $ \mu = A_{mm} $, however, has no effect either, since $ A_{ mm } =0 $. Thus it's clear that in the worst case, the QR algorithm with the Rayleigh quotient shift may fail.  

The problem arises because of the symmetry of the eigenvalues. One eigenvalue is $+1$ and the other is $-1$, so when we attempt to improve the eigenvalue estimate $o$, the tendency to favor each eigenvalue is equal, and the estimate is not improved. What is needed is an eigenvalue estimate that can break the symmetry. One such choice is defined as follows. Let $ B $ denote the lower-rightmost $ 2 \times 2$ submatrix of $ A^{(k)} $: 
\[
    B = \begin{bmatrix}[] 
        a_{m-1} &  b_{m-1} \\
        b_{m-1} &  a_{m} \\
    \end{bmatrix}.  
\]
The \textbf{Wilkinson shift} is defined as that eigenvalue of $ B $ that is closer to $ a_m $, where in the case of a tie, one of the two eigenvalues of $ B $ is chosen arbitrarily. A numerically stable for the Wilkinson shift is 
\begin{equation}
\label{eq: wilkinson shift}
    \mu = a_m - \sign(\delta ) b_{m-1}^{2} / \left( |\delta | + \sqrt{\delta ^{2} + b_{m-1}^{2} }  \right) , 
\end{equation}
where $ \delta = (a_{m-1}-a_m) /2 $. If $\delta =0$, $ \sign (\delta ) $ can be arbitrarily set equal to $ 1 $ or $ -1 $. 

Like the Rayleigh quotient shift, the Wilkinson shift achieves cubic convergence in the generic case. Moreover, it can be shown that it achieves at least quadratic convergence in the worst case. In particular, the QR algorithm with the Wilkinson shift always converges (in exact arithmetic). 

For this example, the Wilkinson shift is either $ +1 $ or $ -1 $. Thus the symmetry is broken, and convergence takes place in one step.  

\section{Stability and Accuracy} 
As one might expect from its use of orthogonal matrices, the QR algorithm is backward stable. As in previous chapter, the simplest way to formulate this result is to let $ \tilde \Lambda  $ denote the diagonalization of $ A $ as computed in floating point arithmetic, and $ \tilde Q $ the exact orthogonal matrix associated with the product of all numerically computed Householder reflections. 


%────────────────────────────────────────
\begin{theorem}
[Backstab of QR for eig]
\label{thm: Backstab of QR for eig}
Let a real, symmetric, tridiagonal matrix $ A\in \RR^{m \times  m} $ be diagonalized by the QR algorithm (Algo~\ref{Algo 28.2}) on a computer satisfying \eqref{eq: fler} and \eqref{eq:flop}, and let $ \tilde \Lambda  $ and $ \tilde Q  $ be defined as indicated above. Then we have 
\begin{equation}
\label{eq: Backstab of QR for eig}
    \tilde Q \tilde \Lambda  \tilde Q ^{*} = A + \delta A, \quad \frac{\|\delta A\|}{\|A\|} = O(\mep)
\end{equation}
for some $ \delta A \in \CC^{m \times  m} $. 
\end{theorem}
%────────────────────────────────────────

Besides, we have some further results that the computed eigenvalues $ \tilde \lambda _j $ satisfy 
\begin{equation}
\label{eq: err eigenvalue QR}
    \frac{|\tilde \lambda _j -\lambda _j|}{\|A\|} = O(\mep). 
\end{equation}
This is not a bad result at all for an algorithm that requires just $\sim \frac{4}{3}m^3$ flops, two thirds the cost of computing the product of a part of $m\times m$ matrices! 


